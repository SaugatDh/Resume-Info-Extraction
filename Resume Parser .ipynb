{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPy7xQZYD1W6LSPaC0hvQlC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"07c34b0a"},"source":["**Reasoning**:\n","Install the gradio library using pip.\n","\n"]},{"cell_type":"code","metadata":{"id":"2d457c35","executionInfo":{"status":"ok","timestamp":1750056212402,"user_tz":-345,"elapsed":652,"user":{"displayName":"Saugat Dhungana","userId":"01702275730195126745"}}},"source":["import fitz  # PyMuPDF\n","import re\n","import spacy\n","from spacy.matcher import Matcher\n","\n","# Load spaCy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# ========== PDF Text Extraction using fitz ==========\n","def extract_text_from_pdf(pdf_path):\n","    doc = fitz.open(pdf_path)\n","    text = \"\"\n","    for page in doc:\n","        text += page.get_text()\n","    return text\n","\n","# ========== Cleaning Broken Lines ==========\n","def clean_text(text):\n","    lines = text.split('\\n')\n","    cleaned_lines = []\n","    i = 0\n","    while i < len(lines):\n","        if i < len(lines)-1 and lines[i].istitle() and lines[i+1].istitle():\n","            cleaned_lines.append(lines[i] + \" \" + lines[i+1])\n","            i += 2\n","        else:\n","            cleaned_lines.append(lines[i])\n","            i += 1\n","    return \"\\n\".join(cleaned_lines)\n","\n","# ========== Extract Name ==========\n","def extract_name(text):\n","    doc = nlp(text)\n","    matcher = Matcher(nlp.vocab)\n","    matcher.add(\"NAME\", [[{'POS': 'PROPN'}, {'POS': 'PROPN'}]])\n","    matches = matcher(doc)\n","    for match_id, start, end in matches:\n","        span = doc[start:end]\n","        return span.text\n","    return None\n","\n","# ========== Extract Email ==========\n","def extract_email(text):\n","    # Normalize whitespace and remove newline breaks that might split emails\n","    cleaned_text = re.sub(r'[\\s\\n]+', '', text)\n","\n","    # Try a relaxed pattern to capture email-like strings\n","    pattern = r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\"\n","    emails = re.findall(pattern, cleaned_text)\n","    return emails[0] if emails else None\n","\n","\n","# ========== Extract Phone ==========\n","def extract_phone(text):\n","    cleaned_text = ' '.join(text.split())  # Normalize whitespace\n","    phones = re.findall(r\"(?:\\+977[-.\\s]?)?[9][6-9]\\d{7,8}\", cleaned_text)\n","    return phones[0] if phones else None\n","\n","# ========== Extract Skills ==========\n","def extract_skills(text, skills_list):\n","    found = []\n","    for skill in skills_list:\n","        if re.search(rf\"\\b{re.escape(skill)}\\b\", text, re.IGNORECASE):\n","            found.append(skill)\n","    return found\n","\n","# ========== Extract Education ==========\n","def extract_education(text):\n","    pattern = r\"(?i)(Bachelors?|Bachelorâ€™s?|Technical Education).+?(?:\\n|$)\"\n","    return [match.strip() for match in re.findall(pattern, text)]\n","\n","# ========== Main Program ==========\n","def extract_resume_info(pdf_path):\n","    \"\"\"\n","    Extracts resume information from a PDF file.\n","\n","    Args:\n","        pdf_path: The path to the PDF file.\n","\n","    Returns:\n","        A dictionary containing the extracted information or an error message.\n","    \"\"\"\n","    try:\n","        raw_text = extract_text_from_pdf(pdf_path)\n","        text = clean_text(raw_text)\n","\n","        # Define skill keywords - keeping this inside the function as it's specific to the extraction logic\n","        skills_list = [\n","            \"React\", \"JavaScript\", \"Python\", \"Machine Learning\", \"AI\", \"NLP\", \"Laravel\",\n","            \"YOLOv8\", \"Frontend Development\", \"Backend Development\", \"IOT Development\"\n","        ]\n","\n","        name = extract_name(text)\n","        email = extract_email(text)\n","        phone = extract_phone(text)\n","        skills = extract_skills(text, skills_list)\n","        education = extract_education(text)\n","\n","        extracted_info = {\n","            'Name': name,\n","            'Email': email,\n","            'Phone': phone,\n","            'Skills': skills,\n","            'Education': education\n","        }\n","\n","        return extracted_info\n","    except Exception as e:\n","        return {\"error\": str(e)}\n","\n","# Example usage (optional, for testing)\n","# resume_path = \"Saugat-Dhungana-cv.pdf\"\n","# extracted_data = extract_resume_info(resume_path)\n","# print(extracted_data)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"d9d1825e","executionInfo":{"status":"ok","timestamp":1750056224551,"user_tz":-345,"elapsed":8471,"user":{"displayName":"Saugat Dhungana","userId":"01702275730195126745"}}},"source":["import gradio as gr\n","\n","# Define the Gradio interface\n","iface = gr.Interface(\n","    fn=extract_resume_info,\n","    inputs=gr.File(label=\"Upload PDF Resume\"),\n","    outputs=gr.JSON(label=\"Extracted Resume Information\"),\n","    title=\"Resume Information Extractor\",\n","    description=\"Upload a PDF resume to extract key information like name, email, phone, skills, and education.\"\n",")\n","\n","# To launch the interface, you would typically use:\n","# iface.launch()\n","# However, for this subtask, we only need to define the interface."],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":644},"id":"94430573","executionInfo":{"status":"ok","timestamp":1750056226214,"user_tz":-345,"elapsed":1655,"user":{"displayName":"Saugat Dhungana","userId":"01702275730195126745"}},"outputId":"32e52ce7-9bd4-4778-e9f5-1c8a9656526b"},"source":["iface.launch()"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://8e45ee90e74b85dc73.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://8e45ee90e74b85dc73.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"4c5f16eb"},"source":["## Summary:\n","\n","### Data Analysis Key Findings\n","\n","*   The `gradio` library was already installed in the environment before the process began.\n","*   A Python function `extract_resume_info` was successfully created to encapsulate the PDF text extraction and information parsing logic.\n","*   A Gradio interface was successfully defined using `gradio.Interface`, featuring a file upload input for PDF files and a JSON output to display extracted information.\n","*   The Gradio application was successfully launched and made publicly accessible via a URL, as the environment was detected as a hosted notebook.\n","\n","### Insights or Next Steps\n","\n","*   The created Gradio application provides a user-friendly interface for extracting structured data from PDF resumes, which can be valuable for recruitment or database population tasks.\n","*   Further development could involve improving the robustness of the extraction functions to handle a wider variety of resume formats and potentially adding more extraction fields like work experience or projects.\n"]}]}